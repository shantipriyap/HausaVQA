# HausaVQA

This repository contains baseline code for the Hausa Visual Question Answer (HausaVQA) dataset, which is the first dataset suitable for VQA and multimodal research in Hausa language. The NLP use cases applied to HaVQA dataset include:

* [Text-Only Machine Translation (T2T)](https://github.com/shantipriyap/HausaVQA/tree/main/T2T)
* [Multimodal Machine Translation (MMT)](https://github.com/shantipriyap/HausaVQA/tree/main/MMT)
* [Visual Question Elicitation (VQE)]()
* [Visual Question Answering (VQA)](https://github.com/shantipriyap/HausaVQA/tree/main/VQA)


## Contributors

* Shantipriya Parida, Silo AI, Helsinki, Finland
* Idris Abdulmumin, Ahmadu Bello University, Zaria, Nigeria
* Shamsuddeen Hassan Muhammad, University of Porto, Portugal
* Aneesh Bose, Microsoft, India
* Guneet Singh Kohli, GreyOrange, India
* Ibrahim Sa'id Ahmad, Bayero University, Kano, Nigeria 
* Ketan Kotwal, Idiap Research Institute, Switzerland
* Sayan Deb Sarkar, ETH Zurich, Switzerland
* Ondřej Bojar, UFAL, Charles University, Prague, Czech Republic
* Habeebah Adamu Kakudi, Bayero University, Kano, Nigeria


### Citation Information

If you find this helpful repository, please consider giving ⭐ and citing:

```
@article{parida2023havqa,
  title={HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language},
  author={Parida, Shantipriya and Abdulmumin, Idris and Muhammad, Shamsuddeen Hassan and Bose, Aneesh and Kohli, Guneet Singh and Ahmad, Ibrahim Said and Kotwal, Ketan and Sarkar, Sayan Deb and Bojar, Ond{\v{r}}ej and Kakudi, Habeebah Adamu},
  journal={arXiv preprint arXiv:2305.17690},
  year={2023}
}
```
